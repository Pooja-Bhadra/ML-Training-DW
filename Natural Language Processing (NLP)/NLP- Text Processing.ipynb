{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c97d518",
   "metadata": {},
   "source": [
    "## NLP "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5afd2c",
   "metadata": {},
   "source": [
    "### Text processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "527f11d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Text normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e3f28d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"My real hope is that the students are thinking about questions around education, ecology and justice and at the same time working on the degree program each of them is in,” said Michael Duffy EdD ’05, director of USF’s McGrath Institute for Jesuit Catholic Education. He hopes they will take the lessons learned in the immersion with them. “And where do they go from there?\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "559850f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'my real hope is that the students are thinking about questions around education, ecology and justice and at the same time working on the degree program each of them is in,” said michael duffy edd ’05, director of usf’s mcgrath institute for jesuit catholic education. he hopes they will take the lessons learned in the immersion with them. “and where do they go from there?'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert to lower case\n",
    "\n",
    "text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0b77bc8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'My real hope is that the students are thinking about questions around education, ecology and justice and at the same time working on the degree program each of them is in,” said Michael Duffy EdD ’05, director of USF’s McGrath Institute for Jesuit Catholic Education He hopes they will take the lessons learned in the immersion with them “And where do they go from there?'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Replace punctuation marks with nothing\n",
    "\n",
    "text1= text.replace('.', '')\n",
    "text1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fb042979",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'My real hope is that the students are thinking about questions around education ecology and justice and at the same time working on the degree program each of them is in” said Michael Duffy EdD ’05 director of USF’s McGrath Institute for Jesuit Catholic Education He hopes they will take the lessons learned in the immersion with them “And where do they go from there?'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text2 = text1.replace(',', '')\n",
    "text2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5d5cb270",
   "metadata": {},
   "outputs": [],
   "source": [
    "text3 = text2.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "590edea3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'my real hope is that the students are thinking about questions around education ecology and justice and at the same time working on the degree program each of them is in” said michael duffy edd ’05 director of usf’s mcgrath institute for jesuit catholic education he hopes they will take the lessons learned in the immersion with them “and where do they go from there?'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0860f287",
   "metadata": {},
   "source": [
    "### Text preprocessing using nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "97c5b732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d3a242b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['My real hope is that the students are thinking about questions around education, ecology and justice and at the same time working on the degree program each of them is in,” said Michael Duffy EdD ’05, director of USF’s McGrath Institute for Jesuit Catholic Education.',\n",
       " 'He hopes they will take the lessons learned in the immersion with them.',\n",
       " '“And where do they go from there?']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenization: Converts para to individual sentences or words\n",
    "\n",
    "# Sentence tokenization\n",
    "sentences = nltk.sent_tokenize(text)\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e9ecd028",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['My',\n",
       " 'real',\n",
       " 'hope',\n",
       " 'is',\n",
       " 'that',\n",
       " 'the',\n",
       " 'students',\n",
       " 'are',\n",
       " 'thinking',\n",
       " 'about',\n",
       " 'questions',\n",
       " 'around',\n",
       " 'education',\n",
       " ',',\n",
       " 'ecology',\n",
       " 'and',\n",
       " 'justice',\n",
       " 'and',\n",
       " 'at',\n",
       " 'the',\n",
       " 'same',\n",
       " 'time',\n",
       " 'working',\n",
       " 'on',\n",
       " 'the',\n",
       " 'degree',\n",
       " 'program',\n",
       " 'each',\n",
       " 'of',\n",
       " 'them',\n",
       " 'is',\n",
       " 'in',\n",
       " ',',\n",
       " '”',\n",
       " 'said',\n",
       " 'Michael',\n",
       " 'Duffy',\n",
       " 'EdD',\n",
       " '’',\n",
       " '05',\n",
       " ',',\n",
       " 'director',\n",
       " 'of',\n",
       " 'USF',\n",
       " '’',\n",
       " 's',\n",
       " 'McGrath',\n",
       " 'Institute',\n",
       " 'for',\n",
       " 'Jesuit',\n",
       " 'Catholic',\n",
       " 'Education',\n",
       " '.',\n",
       " 'He',\n",
       " 'hopes',\n",
       " 'they',\n",
       " 'will',\n",
       " 'take',\n",
       " 'the',\n",
       " 'lessons',\n",
       " 'learned',\n",
       " 'in',\n",
       " 'the',\n",
       " 'immersion',\n",
       " 'with',\n",
       " 'them',\n",
       " '.',\n",
       " '“',\n",
       " 'And',\n",
       " 'where',\n",
       " 'do',\n",
       " 'they',\n",
       " 'go',\n",
       " 'from',\n",
       " 'there',\n",
       " '?']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# word tokenization\n",
    "words = nltk.word_tokenize(text)\n",
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6802ab83",
   "metadata": {},
   "source": [
    "### Stemming and Stop Words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b80c22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer   \n",
    "from nltk.corpus import stopwords "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "64736415",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List the stopwords in English language\n",
    "\n",
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4008738b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['au',\n",
       " 'aux',\n",
       " 'avec',\n",
       " 'ce',\n",
       " 'ces',\n",
       " 'dans',\n",
       " 'de',\n",
       " 'des',\n",
       " 'du',\n",
       " 'elle',\n",
       " 'en',\n",
       " 'et',\n",
       " 'eux',\n",
       " 'il',\n",
       " 'ils',\n",
       " 'je',\n",
       " 'la',\n",
       " 'le',\n",
       " 'les',\n",
       " 'leur',\n",
       " 'lui',\n",
       " 'ma',\n",
       " 'mais',\n",
       " 'me',\n",
       " 'même',\n",
       " 'mes',\n",
       " 'moi',\n",
       " 'mon',\n",
       " 'ne',\n",
       " 'nos',\n",
       " 'notre',\n",
       " 'nous',\n",
       " 'on',\n",
       " 'ou',\n",
       " 'par',\n",
       " 'pas',\n",
       " 'pour',\n",
       " 'qu',\n",
       " 'que',\n",
       " 'qui',\n",
       " 'sa',\n",
       " 'se',\n",
       " 'ses',\n",
       " 'son',\n",
       " 'sur',\n",
       " 'ta',\n",
       " 'te',\n",
       " 'tes',\n",
       " 'toi',\n",
       " 'ton',\n",
       " 'tu',\n",
       " 'un',\n",
       " 'une',\n",
       " 'vos',\n",
       " 'votre',\n",
       " 'vous',\n",
       " 'c',\n",
       " 'd',\n",
       " 'j',\n",
       " 'l',\n",
       " 'à',\n",
       " 'm',\n",
       " 'n',\n",
       " 's',\n",
       " 't',\n",
       " 'y',\n",
       " 'été',\n",
       " 'étée',\n",
       " 'étées',\n",
       " 'étés',\n",
       " 'étant',\n",
       " 'étante',\n",
       " 'étants',\n",
       " 'étantes',\n",
       " 'suis',\n",
       " 'es',\n",
       " 'est',\n",
       " 'sommes',\n",
       " 'êtes',\n",
       " 'sont',\n",
       " 'serai',\n",
       " 'seras',\n",
       " 'sera',\n",
       " 'serons',\n",
       " 'serez',\n",
       " 'seront',\n",
       " 'serais',\n",
       " 'serait',\n",
       " 'serions',\n",
       " 'seriez',\n",
       " 'seraient',\n",
       " 'étais',\n",
       " 'était',\n",
       " 'étions',\n",
       " 'étiez',\n",
       " 'étaient',\n",
       " 'fus',\n",
       " 'fut',\n",
       " 'fûmes',\n",
       " 'fûtes',\n",
       " 'furent',\n",
       " 'sois',\n",
       " 'soit',\n",
       " 'soyons',\n",
       " 'soyez',\n",
       " 'soient',\n",
       " 'fusse',\n",
       " 'fusses',\n",
       " 'fût',\n",
       " 'fussions',\n",
       " 'fussiez',\n",
       " 'fussent',\n",
       " 'ayant',\n",
       " 'ayante',\n",
       " 'ayantes',\n",
       " 'ayants',\n",
       " 'eu',\n",
       " 'eue',\n",
       " 'eues',\n",
       " 'eus',\n",
       " 'ai',\n",
       " 'as',\n",
       " 'avons',\n",
       " 'avez',\n",
       " 'ont',\n",
       " 'aurai',\n",
       " 'auras',\n",
       " 'aura',\n",
       " 'aurons',\n",
       " 'aurez',\n",
       " 'auront',\n",
       " 'aurais',\n",
       " 'aurait',\n",
       " 'aurions',\n",
       " 'auriez',\n",
       " 'auraient',\n",
       " 'avais',\n",
       " 'avait',\n",
       " 'avions',\n",
       " 'aviez',\n",
       " 'avaient',\n",
       " 'eut',\n",
       " 'eûmes',\n",
       " 'eûtes',\n",
       " 'eurent',\n",
       " 'aie',\n",
       " 'aies',\n",
       " 'ait',\n",
       " 'ayons',\n",
       " 'ayez',\n",
       " 'aient',\n",
       " 'eusse',\n",
       " 'eusses',\n",
       " 'eût',\n",
       " 'eussions',\n",
       " 'eussiez',\n",
       " 'eussent']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List the stopwords in French language\n",
    "\n",
    "stopwords.words('french')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "10e01c7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aber',\n",
       " 'alle',\n",
       " 'allem',\n",
       " 'allen',\n",
       " 'aller',\n",
       " 'alles',\n",
       " 'als',\n",
       " 'also',\n",
       " 'am',\n",
       " 'an',\n",
       " 'ander',\n",
       " 'andere',\n",
       " 'anderem',\n",
       " 'anderen',\n",
       " 'anderer',\n",
       " 'anderes',\n",
       " 'anderm',\n",
       " 'andern',\n",
       " 'anderr',\n",
       " 'anders',\n",
       " 'auch',\n",
       " 'auf',\n",
       " 'aus',\n",
       " 'bei',\n",
       " 'bin',\n",
       " 'bis',\n",
       " 'bist',\n",
       " 'da',\n",
       " 'damit',\n",
       " 'dann',\n",
       " 'der',\n",
       " 'den',\n",
       " 'des',\n",
       " 'dem',\n",
       " 'die',\n",
       " 'das',\n",
       " 'dass',\n",
       " 'daß',\n",
       " 'derselbe',\n",
       " 'derselben',\n",
       " 'denselben',\n",
       " 'desselben',\n",
       " 'demselben',\n",
       " 'dieselbe',\n",
       " 'dieselben',\n",
       " 'dasselbe',\n",
       " 'dazu',\n",
       " 'dein',\n",
       " 'deine',\n",
       " 'deinem',\n",
       " 'deinen',\n",
       " 'deiner',\n",
       " 'deines',\n",
       " 'denn',\n",
       " 'derer',\n",
       " 'dessen',\n",
       " 'dich',\n",
       " 'dir',\n",
       " 'du',\n",
       " 'dies',\n",
       " 'diese',\n",
       " 'diesem',\n",
       " 'diesen',\n",
       " 'dieser',\n",
       " 'dieses',\n",
       " 'doch',\n",
       " 'dort',\n",
       " 'durch',\n",
       " 'ein',\n",
       " 'eine',\n",
       " 'einem',\n",
       " 'einen',\n",
       " 'einer',\n",
       " 'eines',\n",
       " 'einig',\n",
       " 'einige',\n",
       " 'einigem',\n",
       " 'einigen',\n",
       " 'einiger',\n",
       " 'einiges',\n",
       " 'einmal',\n",
       " 'er',\n",
       " 'ihn',\n",
       " 'ihm',\n",
       " 'es',\n",
       " 'etwas',\n",
       " 'euer',\n",
       " 'eure',\n",
       " 'eurem',\n",
       " 'euren',\n",
       " 'eurer',\n",
       " 'eures',\n",
       " 'für',\n",
       " 'gegen',\n",
       " 'gewesen',\n",
       " 'hab',\n",
       " 'habe',\n",
       " 'haben',\n",
       " 'hat',\n",
       " 'hatte',\n",
       " 'hatten',\n",
       " 'hier',\n",
       " 'hin',\n",
       " 'hinter',\n",
       " 'ich',\n",
       " 'mich',\n",
       " 'mir',\n",
       " 'ihr',\n",
       " 'ihre',\n",
       " 'ihrem',\n",
       " 'ihren',\n",
       " 'ihrer',\n",
       " 'ihres',\n",
       " 'euch',\n",
       " 'im',\n",
       " 'in',\n",
       " 'indem',\n",
       " 'ins',\n",
       " 'ist',\n",
       " 'jede',\n",
       " 'jedem',\n",
       " 'jeden',\n",
       " 'jeder',\n",
       " 'jedes',\n",
       " 'jene',\n",
       " 'jenem',\n",
       " 'jenen',\n",
       " 'jener',\n",
       " 'jenes',\n",
       " 'jetzt',\n",
       " 'kann',\n",
       " 'kein',\n",
       " 'keine',\n",
       " 'keinem',\n",
       " 'keinen',\n",
       " 'keiner',\n",
       " 'keines',\n",
       " 'können',\n",
       " 'könnte',\n",
       " 'machen',\n",
       " 'man',\n",
       " 'manche',\n",
       " 'manchem',\n",
       " 'manchen',\n",
       " 'mancher',\n",
       " 'manches',\n",
       " 'mein',\n",
       " 'meine',\n",
       " 'meinem',\n",
       " 'meinen',\n",
       " 'meiner',\n",
       " 'meines',\n",
       " 'mit',\n",
       " 'muss',\n",
       " 'musste',\n",
       " 'nach',\n",
       " 'nicht',\n",
       " 'nichts',\n",
       " 'noch',\n",
       " 'nun',\n",
       " 'nur',\n",
       " 'ob',\n",
       " 'oder',\n",
       " 'ohne',\n",
       " 'sehr',\n",
       " 'sein',\n",
       " 'seine',\n",
       " 'seinem',\n",
       " 'seinen',\n",
       " 'seiner',\n",
       " 'seines',\n",
       " 'selbst',\n",
       " 'sich',\n",
       " 'sie',\n",
       " 'ihnen',\n",
       " 'sind',\n",
       " 'so',\n",
       " 'solche',\n",
       " 'solchem',\n",
       " 'solchen',\n",
       " 'solcher',\n",
       " 'solches',\n",
       " 'soll',\n",
       " 'sollte',\n",
       " 'sondern',\n",
       " 'sonst',\n",
       " 'über',\n",
       " 'um',\n",
       " 'und',\n",
       " 'uns',\n",
       " 'unsere',\n",
       " 'unserem',\n",
       " 'unseren',\n",
       " 'unser',\n",
       " 'unseres',\n",
       " 'unter',\n",
       " 'viel',\n",
       " 'vom',\n",
       " 'von',\n",
       " 'vor',\n",
       " 'während',\n",
       " 'war',\n",
       " 'waren',\n",
       " 'warst',\n",
       " 'was',\n",
       " 'weg',\n",
       " 'weil',\n",
       " 'weiter',\n",
       " 'welche',\n",
       " 'welchem',\n",
       " 'welchen',\n",
       " 'welcher',\n",
       " 'welches',\n",
       " 'wenn',\n",
       " 'werde',\n",
       " 'werden',\n",
       " 'wie',\n",
       " 'wieder',\n",
       " 'will',\n",
       " 'wir',\n",
       " 'wird',\n",
       " 'wirst',\n",
       " 'wo',\n",
       " 'wollen',\n",
       " 'wollte',\n",
       " 'würde',\n",
       " 'würden',\n",
       " 'zu',\n",
       " 'zum',\n",
       " 'zur',\n",
       " 'zwar',\n",
       " 'zwischen']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List the stopwords in German language\n",
    "\n",
    "stopwords.words('german')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "151203a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['my real hope student think question around educ , ecolog justic time work degre program , ” said michael duffi edd ’ 05 , director usf ’ mcgrath institut jesuit cathol educ . he hope take lesson learn immers . “ and go ?', 'my real hope student think question around educ , ecolog justic time work degre program , ” said michael duffi edd ’ 05 , director usf ’ mcgrath institut jesuit cathol educ . he hope take lesson learn immers . “ and go ?', 'my real hope student think question around educ , ecolog justic time work degre program , ” said michael duffi edd ’ 05 , director usf ’ mcgrath institut jesuit cathol educ . he hope take lesson learn immers . “ and go ?']\n"
     ]
    }
   ],
   "source": [
    "## Stemming\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    words = nltk.word_tokenize(text)        #considering the words in each sentence i.e. word tokenization\n",
    "    \n",
    "    # if the word in sentence does not belong to stop word, find its stem\n",
    "    words = [stemmer.stem(word) for word in words if word not in set(stopwords.words('english'))]\n",
    "    \n",
    "    # join these words to replace the old sentence\n",
    "    sentences[i] = ' '.join(words) \n",
    "\n",
    "print(sentences)\n",
    "\n",
    "## Drawback of stemming:\n",
    "# Produces intermediate words that might not have any meaning. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b287a0e",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ac219ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ad2bf2fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['My real hope is that the students are thinking about questions around education, ecology and justice and at the same time working on the degree program each of them is in,” said Michael Duffy EdD ’05, director of USF’s McGrath Institute for Jesuit Catholic Education.',\n",
       " 'He hopes they will take the lessons learned in the immersion with them.',\n",
       " '“And where do they go from there?']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences1 = nltk.sent_tokenize(text)\n",
    "sentences1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7cab9de3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['My real hope student thinking question around education , ecology justice time working degree program , ” said Michael Duffy EdD ’ 05 , director USF ’ McGrath Institute Jesuit Catholic Education . He hope take lesson learned immersion . “ And go ?', 'My real hope student thinking question around education , ecology justice time working degree program , ” said Michael Duffy EdD ’ 05 , director USF ’ McGrath Institute Jesuit Catholic Education . He hope take lesson learned immersion . “ And go ?', 'My real hope student thinking question around education , ecology justice time working degree program , ” said Michael Duffy EdD ’ 05 , director USF ’ McGrath Institute Jesuit Catholic Education . He hope take lesson learned immersion . “ And go ?']\n"
     ]
    }
   ],
   "source": [
    "## Stemming\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "for i in range(len(sentences1)):\n",
    "    words = nltk.word_tokenize(text)        #considering the words in each sentence i.e. word tokenization\n",
    "    \n",
    "    # if the word in sentence does not belong to stop word, find its lemmatizer\n",
    "    words = [lemmatizer.lemmatize(word) for word in words if word not in set(stopwords.words('english'))]\n",
    "    \n",
    "    # join these words to replace the old sentence\n",
    "    sentences1[i] = ' '.join(words) \n",
    "\n",
    "print(sentences1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68e8f3b",
   "metadata": {},
   "source": [
    "### Term Frequency and Inverse Document Frequency (TF and IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a5982475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: clean the data\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9b55b26b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['My real hope is that the students are thinking about questions around education, ecology and justice and at the same time working on the degree program each of them is in,” said Michael Duffy EdD ’05, director of USF’s McGrath Institute for Jesuit Catholic Education.',\n",
       " 'He hopes they will take the lessons learned in the immersion with them.',\n",
       " '“And where do they go from there?']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = nltk.sent_tokenize(text)\n",
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "374e0d6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['real hope student thinking question around education ecology justice time working degree program said michael duffy edd director usf mcgrath institute jesuit catholic education', 'hope take lesson learned immersion', 'go']\n"
     ]
    }
   ],
   "source": [
    "reduced_sentence = []\n",
    "for i in range(len(sentence)):\n",
    "    review = re.sub('[^a-zA-Z]', ' ', sentence[i])  # keeps only letters and removes punctuations and numbers\n",
    "    review = review.lower()                         # changes everything to lower case\n",
    "    review = review.split()                         # get list of words\n",
    "    review = [lemmatizer.lemmatize(word) for word in review if word not in set(stopwords.words('english'))]\n",
    "    review = ' '.join(review) \n",
    "    reduced_sentence.append(review)\n",
    "print(reduced_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "38ffb4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: TF-IDF\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e8a8a170",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = TfidfVectorizer()\n",
    "X = cv.fit_transform(reduced_sentence).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "397a3725",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.19772579, 0.19772579, 0.19772579, 0.19772579, 0.19772579,\n",
       "        0.19772579, 0.19772579, 0.39545158, 0.        , 0.15037556,\n",
       "        0.        , 0.19772579, 0.19772579, 0.19772579, 0.        ,\n",
       "        0.        , 0.19772579, 0.19772579, 0.19772579, 0.19772579,\n",
       "        0.19772579, 0.19772579, 0.19772579, 0.        , 0.19772579,\n",
       "        0.19772579, 0.19772579, 0.19772579],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.35543247,\n",
       "        0.46735098, 0.        , 0.        , 0.        , 0.46735098,\n",
       "        0.46735098, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.46735098, 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 1.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7b742b",
   "metadata": {},
   "source": [
    "### Bag of Words  (BOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "759ff3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Same as TF-IDF (Clean the data)\n",
    "\n",
    "# Step 2: Create Bag of Words Model\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5e621c38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 1, 1, 1, 1, 2, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,\n",
       "        1, 0, 1, 1, 1, 1],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "        0, 1, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bog = CountVectorizer()\n",
    "X_bog = bog.fit_transform(reduced_sentence).toarray()\n",
    "X_bog"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f991eaf",
   "metadata": {},
   "source": [
    "### Ngrams "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "fa7d9d01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['my real hope is that the students are thinking about questions around education ecology and justice and at the same time working on the degree program each of them is in said michael duffy edd director of usf s mcgrath institute for jesuit catholic education', 'he hopes they will take the lessons learned in the immersion with them', 'and where do they go from there']\n"
     ]
    }
   ],
   "source": [
    "# preprocessing and cleaning text\n",
    "\n",
    "clean_sentence = []\n",
    "for i in range(len(sentence)):\n",
    "    review = re.sub('[^a-zA-Z]', ' ', sentence[i])  # keeps only letters and removes punctuations and numbers\n",
    "    review = review.lower()                         # changes everything to lower case\n",
    "    review = review.split()                         # get list of words\n",
    "    review = ' '.join(review) \n",
    "    clean_sentence.append(review)\n",
    "print(clean_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1e4ade9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0,\n",
       "        0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,\n",
       "        1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0],\n",
       "       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NGrams using sklearn \n",
    "\n",
    "n_grams = CountVectorizer(ngram_range=(2,2))\n",
    "ng_model = n_grams.fit_transform(clean_sentence).toarray()\n",
    "ng_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "85cad082",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "782335a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   about questions  and at  and justice  and where  are thinking  \\\n",
      "0                1       1            1          0             1   \n",
      "1                0       0            0          0             0   \n",
      "2                0       0            0          1             0   \n",
      "\n",
      "   around education  at the  catholic education  degree program  director of  \\\n",
      "0                 1       1                   1               1            1   \n",
      "1                 0       0                   0               0            0   \n",
      "2                 0       0                   0               0            0   \n",
      "\n",
      "   ...  them is  they go  they will  thinking about  time working  \\\n",
      "0  ...        1        0          0               1             1   \n",
      "1  ...        0        0          1               0             0   \n",
      "2  ...        0        1          0               0             0   \n",
      "\n",
      "   usf mcgrath  where do  will take  with them  working on  \n",
      "0            1         0          0          0           1  \n",
      "1            0         0          1          1           0  \n",
      "2            0         1          0          0           0  \n",
      "\n",
      "[3 rows x 61 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/abhadra/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(ng_model, columns=n_grams.get_feature_names())\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "065ec7ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method CountVectorizer.get_feature_names of CountVectorizer(ngram_range=(2, 2))>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_grams.get_feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23bf49fd",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "d6acd075",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "07bd2877",
   "metadata": {},
   "outputs": [],
   "source": [
    "para = \"My real hope is that the students are thinking about questions around education, ecology and justice and at the same time working on the degree program each of them is in,” said Michael Duffy EdD ’05, director of USF’s McGrath Institute for Jesuit Catholic Education. He hopes they will take the lessons learned in the immersion with them. “And where do they go from there?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "abe369b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pro_para = re.sub('[^a-zA-Z]', ' ', para)\n",
    "pro_para = pro_para.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "6ec6f4de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['my',\n",
       "  'real',\n",
       "  'hope',\n",
       "  'is',\n",
       "  'that',\n",
       "  'the',\n",
       "  'students',\n",
       "  'are',\n",
       "  'thinking',\n",
       "  'about',\n",
       "  'questions',\n",
       "  'around',\n",
       "  'education',\n",
       "  'ecology',\n",
       "  'and',\n",
       "  'justice',\n",
       "  'and',\n",
       "  'at',\n",
       "  'the',\n",
       "  'same',\n",
       "  'time',\n",
       "  'working',\n",
       "  'on',\n",
       "  'the',\n",
       "  'degree',\n",
       "  'program',\n",
       "  'each',\n",
       "  'of',\n",
       "  'them',\n",
       "  'is',\n",
       "  'in',\n",
       "  'said',\n",
       "  'michael',\n",
       "  'duffy',\n",
       "  'edd',\n",
       "  'director',\n",
       "  'of',\n",
       "  'usf',\n",
       "  's',\n",
       "  'mcgrath',\n",
       "  'institute',\n",
       "  'for',\n",
       "  'jesuit',\n",
       "  'catholic',\n",
       "  'education',\n",
       "  'he',\n",
       "  'hopes',\n",
       "  'they',\n",
       "  'will',\n",
       "  'take',\n",
       "  'the',\n",
       "  'lessons',\n",
       "  'learned',\n",
       "  'in',\n",
       "  'the',\n",
       "  'immersion',\n",
       "  'with',\n",
       "  'them',\n",
       "  'and',\n",
       "  'where',\n",
       "  'do',\n",
       "  'they',\n",
       "  'go',\n",
       "  'from',\n",
       "  'there']]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_w2v = nltk.sent_tokenize(pro_para) \n",
    "sentences_w2v = [nltk.word_tokenize(sentence) for sentence in sentences_w2v]\n",
    "sentences_w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "afe0ec21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['real', 'hope', 'students', 'thinking', 'questions', 'around', 'education', 'ecology', 'justice', 'time', 'working', 'degree', 'program', 'said', 'michael', 'duffy', 'edd', 'director', 'usf', 'mcgrath', 'institute', 'jesuit', 'catholic', 'education', 'hopes', 'take', 'lessons', 'learned', 'immersion', 'go']]\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(sentences_w2v)):\n",
    "    sentences_w2v[i] = [word for word in sentences_w2v[i] if word not in stopwords.words('english')]\n",
    "\n",
    "print(sentences_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "30726559",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the Model\n",
    "\n",
    "model = Word2Vec(sentences_w2v, min_count=1)   #min_count=2: if the word is less than 2 times, remove it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "0013e772",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "similar = model.wv.most_similar('education')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "a7fda267",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('duffy', 0.21893946826457977),\n",
       " ('justice', 0.2161060869693756),\n",
       " ('real', 0.19535166025161743),\n",
       " ('learned', 0.09945564717054367),\n",
       " ('degree', 0.09308914840221405),\n",
       " ('around', 0.09296789765357971),\n",
       " ('mcgrath', 0.08420460671186447),\n",
       " ('working', 0.07967584580183029),\n",
       " ('institute', 0.0643773302435875),\n",
       " ('time', 0.06284185498952866)]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9b64fe",
   "metadata": {},
   "source": [
    "### GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04861ccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in /Users/abhadra/opt/anaconda3/lib/python3.9/site-packages (2.13.1)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d421788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: protobuf in /Users/abhadra/opt/anaconda3/lib/python3.9/site-packages (4.23.4)\n",
      "Collecting protobuf\n",
      "  Obtaining dependency information for protobuf from https://files.pythonhosted.org/packages/fe/f3/957db80e5b9f7fd7df97e5554fdc57919dfad24e89291223fd04a0e3c84f/protobuf-4.24.3-cp37-abi3-macosx_10_9_universal2.whl.metadata\n",
      "  Downloading protobuf-4.24.3-cp37-abi3-macosx_10_9_universal2.whl.metadata (540 bytes)\n",
      "Downloading protobuf-4.24.3-cp37-abi3-macosx_10_9_universal2.whl (409 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m409.4/409.4 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: protobuf\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 4.23.4\n",
      "    Uninstalling protobuf-4.23.4:\n",
      "      Successfully uninstalled protobuf-4.23.4\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tb-nightly 2.15.0a20230902 requires protobuf<4.24,>=3.19.6, but you have protobuf 4.24.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed protobuf-4.24.3\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade protobuf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72434e9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /Users/abhadra/opt/anaconda3/lib/python3.9/site-packages (23.2.1)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "1df0ee26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tf-nightly\n",
      "  Obtaining dependency information for tf-nightly from https://files.pythonhosted.org/packages/f9/cc/98d63f4aceaaf753553c56365da27e152a98d6bc574a295e1c3b6ff8e125/tf_nightly-2.15.0.dev20230904-cp39-cp39-macosx_10_15_x86_64.whl.metadata\n",
      "  Downloading tf_nightly-2.15.0.dev20230904-cp39-cp39-macosx_10_15_x86_64.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /Users/abhadra/opt/anaconda3/lib/python3.9/site-packages (from tf-nightly) (1.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Users/abhadra/opt/anaconda3/lib/python3.9/site-packages (from tf-nightly) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /Users/abhadra/opt/anaconda3/lib/python3.9/site-packages (from tf-nightly) (23.5.26)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /Users/abhadra/opt/anaconda3/lib/python3.9/site-packages (from tf-nightly) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /Users/abhadra/opt/anaconda3/lib/python3.9/site-packages (from tf-nightly) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /Users/abhadra/opt/anaconda3/lib/python3.9/site-packages (from tf-nightly) (3.6.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /Users/abhadra/opt/anaconda3/lib/python3.9/site-packages (from tf-nightly) (16.0.6)\n",
      "Collecting ml-dtypes>=0.2.0 (from tf-nightly)\n",
      "  Obtaining dependency information for ml-dtypes>=0.2.0 from https://files.pythonhosted.org/packages/f0/d8/f602f05db13d187884ddb5ae4e823d333beb28bbd3d12c057450afa3acee/ml_dtypes-0.2.0-cp39-cp39-macosx_10_9_universal2.whl.metadata\n",
      "  Downloading ml_dtypes-0.2.0-cp39-cp39-macosx_10_9_universal2.whl.metadata (20 kB)\n",
      "Requirement already satisfied: numpy>=1.23.5 in /Users/abhadra/opt/anaconda3/lib/python3.9/site-packages (from tf-nightly) (1.24.3)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Users/abhadra/opt/anaconda3/lib/python3.9/site-packages (from tf-nightly) (3.3.0)\n",
      "Requirement already satisfied: packaging in /Users/abhadra/opt/anaconda3/lib/python3.9/site-packages (from tf-nightly) (21.3)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /Users/abhadra/opt/anaconda3/lib/python3.9/site-packages (from tf-nightly) (4.24.2)\n",
      "Requirement already satisfied: setuptools in /Users/abhadra/opt/anaconda3/lib/python3.9/site-packages (from tf-nightly) (61.2.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/abhadra/opt/anaconda3/lib/python3.9/site-packages (from tf-nightly) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Users/abhadra/opt/anaconda3/lib/python3.9/site-packages (from tf-nightly) (2.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /Users/abhadra/opt/anaconda3/lib/python3.9/site-packages (from tf-nightly) (4.1.1)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /Users/abhadra/opt/anaconda3/lib/python3.9/site-packages (from tf-nightly) (1.12.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Users/abhadra/opt/anaconda3/lib/python3.9/site-packages (from tf-nightly) (1.57.0)\n",
      "Collecting tb-nightly~=2.15.0.a (from tf-nightly)\n",
      "  Obtaining dependency information for tb-nightly~=2.15.0.a from https://files.pythonhosted.org/packages/66/9a/b6d21ad7d69ce6f78d57bf4cb6382c2121811deeb128c57da22b042fe147/tb_nightly-2.15.0a20230902-py3-none-any.whl.metadata\n",
      "  Downloading tb_nightly-2.15.0a20230902-py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting tf-estimator-nightly~=2.14.0.dev (from tf-nightly)\n",
      "  Obtaining dependency information for tf-estimator-nightly~=2.14.0.dev from https://files.pythonhosted.org/packages/de/c9/704359f2438c49c3130b7a7c27dedb8e958e9ffa43adb724bae0a57fc564/tf_estimator_nightly-2.14.0.dev2023080308-py2.py3-none-any.whl.metadata\n",
      "  Downloading tf_estimator_nightly-2.14.0.dev2023080308-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting keras-nightly~=2.15.0.dev (from tf-nightly)\n",
      "  Obtaining dependency information for keras-nightly~=2.15.0.dev from https://files.pythonhosted.org/packages/f2/d6/ce957eb22877ea24a41042a24b217d1438dd466d558972c267abf4e0811f/keras_nightly-2.15.0.dev2023090507-py3-none-any.whl.metadata\n",
      "  Downloading keras_nightly-2.15.0.dev2023090507-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /Users/abhadra/opt/anaconda3/lib/python3.9/site-packages (from tf-nightly) (0.33.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Users/abhadra/opt/anaconda3/lib/python3.9/site-packages (from astunparse>=1.6.0->tf-nightly) (0.37.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /Users/abhadra/opt/anaconda3/lib/python3.9/site-packages (from tb-nightly~=2.15.0.a->tf-nightly) (2.22.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /Users/abhadra/opt/anaconda3/lib/python3.9/site-packages (from tb-nightly~=2.15.0.a->tf-nightly) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/abhadra/opt/anaconda3/lib/python3.9/site-packages (from tb-nightly~=2.15.0.a->tf-nightly) (3.3.4)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 (from tf-nightly)\n",
      "  Obtaining dependency information for protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 from https://files.pythonhosted.org/packages/cb/d3/a164038605494d49acc4f9cda1c0bc200b96382c53edd561387263bb181d/protobuf-4.23.4-cp37-abi3-macosx_10_9_universal2.whl.metadata\n",
      "  Downloading protobuf-4.23.4-cp37-abi3-macosx_10_9_universal2.whl.metadata (540 bytes)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/abhadra/opt/anaconda3/lib/python3.9/site-packages (from tb-nightly~=2.15.0.a->tf-nightly) (2.27.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Users/abhadra/opt/anaconda3/lib/python3.9/site-packages (from tb-nightly~=2.15.0.a->tf-nightly) (0.7.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/abhadra/opt/anaconda3/lib/python3.9/site-packages (from tb-nightly~=2.15.0.a->tf-nightly) (2.0.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/abhadra/opt/anaconda3/lib/python3.9/site-packages (from packaging->tf-nightly) (3.0.4)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/abhadra/opt/anaconda3/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tb-nightly~=2.15.0.a->tf-nightly) (4.2.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/abhadra/opt/anaconda3/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tb-nightly~=2.15.0.a->tf-nightly) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/abhadra/opt/anaconda3/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tb-nightly~=2.15.0.a->tf-nightly) (4.7.2)\n",
      "Requirement already satisfied: urllib3<2.0 in /Users/abhadra/opt/anaconda3/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tb-nightly~=2.15.0.a->tf-nightly) (1.26.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/abhadra/opt/anaconda3/lib/python3.9/site-packages (from google-auth-oauthlib<1.1,>=0.5->tb-nightly~=2.15.0.a->tf-nightly) (1.3.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/abhadra/opt/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.21.0->tb-nightly~=2.15.0.a->tf-nightly) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/abhadra/opt/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.21.0->tb-nightly~=2.15.0.a->tf-nightly) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/abhadra/opt/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.21.0->tb-nightly~=2.15.0.a->tf-nightly) (3.3)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /Users/abhadra/opt/anaconda3/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tb-nightly~=2.15.0.a->tf-nightly) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/abhadra/opt/anaconda3/lib/python3.9/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tb-nightly~=2.15.0.a->tf-nightly) (3.2.2)\n",
      "Downloading tf_nightly-2.15.0.dev20230904-cp39-cp39-macosx_10_15_x86_64.whl (232.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.7/232.7 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0mm00:01\u001b[0m\n",
      "\u001b[?25hDownloading keras_nightly-2.15.0.dev2023090507-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading ml_dtypes-0.2.0-cp39-cp39-macosx_10_9_universal2.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tb_nightly-2.15.0a20230902-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading protobuf-4.23.4-cp37-abi3-macosx_10_9_universal2.whl (400 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m400.3/400.3 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tf_estimator_nightly-2.14.0.dev2023080308-py2.py3-none-any.whl (440 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m440.9/440.9 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tf-estimator-nightly, protobuf, ml-dtypes, keras-nightly, tb-nightly, tf-nightly\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 4.24.2\n",
      "    Uninstalling protobuf-4.24.2:\n",
      "      Successfully uninstalled protobuf-4.24.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "google-api-core 1.25.1 requires google-auth<2.0dev,>=1.21.1, but you have google-auth 2.22.0 which is incompatible.\n",
      "google-cloud-core 1.7.1 requires google-auth<2.0dev,>=1.24.0, but you have google-auth 2.22.0 which is incompatible.\n",
      "google-cloud-storage 1.31.0 requires google-auth<2.0dev,>=1.11.0, but you have google-auth 2.22.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed keras-nightly-2.15.0.dev2023090507 ml-dtypes-0.2.0 protobuf-4.23.4 tb-nightly-2.15.0a20230902 tf-estimator-nightly-2.14.0.dev2023080308 tf-nightly-2.15.0.dev20230904\n"
     ]
    }
   ],
   "source": [
    "!pip install tf-nightly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "579dd608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /Users/abhadra/opt/anaconda3/lib/python3.9/site-packages (2.13.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /Users/abhadra/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (1.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Users/abhadra/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.1.21 in /Users/abhadra/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (23.5.26)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /Users/abhadra/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /Users/abhadra/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /Users/abhadra/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (3.6.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /Users/abhadra/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (16.0.6)\n",
      "Requirement already satisfied: numpy<=1.24.3,>=1.22 in /Users/abhadra/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (1.24.3)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Users/abhadra/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in /Users/abhadra/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (21.3)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /Users/abhadra/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (4.23.4)\n",
      "Requirement already satisfied: setuptools in /Users/abhadra/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (61.2.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/abhadra/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Users/abhadra/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (2.3.0)\n",
      "Collecting typing-extensions<4.6.0,>=3.6.6 (from tensorflow)\n",
      "  Downloading typing_extensions-4.5.0-py3-none-any.whl (27 kB)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /Users/abhadra/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (1.12.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Users/abhadra/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (1.57.0)\n",
      "Requirement already satisfied: tensorboard<2.14,>=2.13 in /Users/abhadra/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (2.13.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.14,>=2.13.0 in /Users/abhadra/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (2.13.0)\n",
      "Requirement already satisfied: keras<2.14,>=2.13.1 in /Users/abhadra/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (2.13.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /Users/abhadra/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (0.33.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Users/abhadra/opt/anaconda3/lib/python3.9/site-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /Users/abhadra/opt/anaconda3/lib/python3.9/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (2.22.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /Users/abhadra/opt/anaconda3/lib/python3.9/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/abhadra/opt/anaconda3/lib/python3.9/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (3.3.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/abhadra/opt/anaconda3/lib/python3.9/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (2.27.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Users/abhadra/opt/anaconda3/lib/python3.9/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (0.7.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/abhadra/opt/anaconda3/lib/python3.9/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (2.0.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/abhadra/opt/anaconda3/lib/python3.9/site-packages (from packaging->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/abhadra/opt/anaconda3/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (4.2.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/abhadra/opt/anaconda3/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/abhadra/opt/anaconda3/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (4.7.2)\n",
      "Requirement already satisfied: urllib3<2.0 in /Users/abhadra/opt/anaconda3/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (1.26.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/abhadra/opt/anaconda3/lib/python3.9/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/abhadra/opt/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/abhadra/opt/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/abhadra/opt/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (3.3)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /Users/abhadra/opt/anaconda3/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/abhadra/opt/anaconda3/lib/python3.9/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow) (3.2.2)\n",
      "Installing collected packages: typing-extensions\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.7.1\n",
      "    Uninstalling typing_extensions-4.7.1:\n",
      "      Successfully uninstalled typing_extensions-4.7.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "pydantic 2.3.0 requires typing-extensions>=4.6.1, but you have typing-extensions 4.5.0 which is incompatible.\n",
      "pydantic-core 2.6.3 requires typing-extensions!=4.7.0,>=4.6.0, but you have typing-extensions 4.5.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed typing-extensions-4.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "db00d9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f1bbb28",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow.compat'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tokenizer\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msequence\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pad_sequences\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/keras/__init__.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"AUTOGENERATED. DO NOT EDIT.\"\"\"\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __internal__\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m activations\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m applications\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/keras/__internal__/__init__.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"AUTOGENERATED. DO NOT EDIT.\"\"\"\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m backend\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m layers\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m losses\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/keras/__internal__/backend/__init__.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"AUTOGENERATED. DO NOT EDIT.\"\"\"\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _initialize_variables \u001b[38;5;28;01mas\u001b[39;00m initialize_variables\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m track_variable\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/keras/src/__init__.py:21\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Implementation of the Keras API, the high-level API of TensorFlow.\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \n\u001b[1;32m     17\u001b[0m \u001b[38;5;124;03mDetailed documentation and user guides are available at\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;124;03m[keras.io](https://keras.io).\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m distribute\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m models\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minput_layer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Input\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msequential\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sequential\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/keras/src/models/__init__.py:18\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2022 The TensorFlow Authors. All Rights Reserved.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# ==============================================================================\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Keras models API.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Functional\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msequential\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sequential\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtraining\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Model\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/keras/src/engine/functional.py:23\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mitertools\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m backend\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtensor\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m layout_map \u001b[38;5;28;01mas\u001b[39;00m layout_map_lib\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow.compat'"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9d4c2fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# word tokeinze text \n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(clean_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ab751afb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index = tokenizer.word_index   #gets index of word\n",
    "vocab_size = len(word_index)\n",
    "vocab_size \n",
    "\n",
    "## we have 53 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6779496f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "258"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(len(data) for data in clean_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "18ba213b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# padding text data\n",
    "# to convert text into uniform list of features\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(clean_sentence)\n",
    "padded_seq = pad_sequences(sequences, maxlen=258, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ff00e1b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 9, 10, 11,  3, 12,  1, 13, 14, 15, 16, 17, 18,  4, 19,  2, 20,  2,\n",
       "       21,  1, 22, 23, 24, 25,  1, 26, 27, 28,  5,  6,  3,  7, 29, 30, 31,\n",
       "       32, 33,  5, 34, 35, 36, 37, 38, 39, 40,  4,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0], dtype=int32)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_seq[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "107ec4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "8efbb1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding index\n",
    "\n",
    "embedding_dict={}\n",
    "with open('glove.6B.100d.txt', encoding='utf-8') as f:    # loading the data\n",
    "    for line in f:\n",
    "        values=line.split()                               # split text into list\n",
    "        word=values[0]                                    # values will be words\n",
    "        coef=np.asarray(values[1:],'float32')             #\n",
    "        embedding_dict[word]=coef\n",
    "#f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "aed62918",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.030769 ,  0.11993  ,  0.53909  , -0.43696  , -0.73937  ,\n",
       "       -0.15345  ,  0.081126 , -0.38559  , -0.68797  , -0.41632  ,\n",
       "       -0.13183  , -0.24922  ,  0.441    ,  0.085919 ,  0.20871  ,\n",
       "       -0.063582 ,  0.062228 , -0.051234 , -0.13398  ,  1.1418   ,\n",
       "        0.036526 ,  0.49029  , -0.24567  , -0.412    ,  0.12349  ,\n",
       "        0.41336  , -0.48397  , -0.54243  , -0.27787  , -0.26015  ,\n",
       "       -0.38485  ,  0.78656  ,  0.1023   , -0.20712  ,  0.40751  ,\n",
       "        0.32026  , -0.51052  ,  0.48362  , -0.0099498, -0.38685  ,\n",
       "        0.034975 , -0.167    ,  0.4237   , -0.54164  , -0.30323  ,\n",
       "       -0.36983  ,  0.082836 , -0.52538  , -0.064531 , -1.398    ,\n",
       "       -0.14873  , -0.35327  , -0.1118   ,  1.0912   ,  0.095864 ,\n",
       "       -2.8129   ,  0.45238  ,  0.46213  ,  1.6012   , -0.20837  ,\n",
       "       -0.27377  ,  0.71197  , -1.0754   , -0.046974 ,  0.67479  ,\n",
       "       -0.065839 ,  0.75824  ,  0.39405  ,  0.15507  , -0.64719  ,\n",
       "        0.32796  , -0.031748 ,  0.52899  , -0.43886  ,  0.67405  ,\n",
       "        0.42136  , -0.11981  , -0.21777  , -0.29756  , -0.1351   ,\n",
       "        0.59898  ,  0.46529  , -0.58258  , -0.02323  , -1.5442   ,\n",
       "        0.01901  , -0.015877 ,  0.024499 , -0.58017  , -0.67659  ,\n",
       "       -0.040379 , -0.44043  ,  0.083292 ,  0.20035  , -0.75499  ,\n",
       "        0.16918  , -0.26573  , -0.52878  ,  0.17584  ,  1.065    ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_dict['good']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "edcf0d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((vocab_size+1, 100))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embedding_dict.get(word)           # if present, give word or else 0\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "eef2fa45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54, 100)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "0717d3a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 1,\n",
       " 'and': 2,\n",
       " 'is': 3,\n",
       " 'education': 4,\n",
       " 'of': 5,\n",
       " 'them': 6,\n",
       " 'in': 7,\n",
       " 'they': 8,\n",
       " 'my': 9,\n",
       " 'real': 10,\n",
       " 'hope': 11,\n",
       " 'that': 12,\n",
       " 'students': 13,\n",
       " 'are': 14,\n",
       " 'thinking': 15,\n",
       " 'about': 16,\n",
       " 'questions': 17,\n",
       " 'around': 18,\n",
       " 'ecology': 19,\n",
       " 'justice': 20,\n",
       " 'at': 21,\n",
       " 'same': 22,\n",
       " 'time': 23,\n",
       " 'working': 24,\n",
       " 'on': 25,\n",
       " 'degree': 26,\n",
       " 'program': 27,\n",
       " 'each': 28,\n",
       " 'said': 29,\n",
       " 'michael': 30,\n",
       " 'duffy': 31,\n",
       " 'edd': 32,\n",
       " 'director': 33,\n",
       " 'usf': 34,\n",
       " 's': 35,\n",
       " 'mcgrath': 36,\n",
       " 'institute': 37,\n",
       " 'for': 38,\n",
       " 'jesuit': 39,\n",
       " 'catholic': 40,\n",
       " 'he': 41,\n",
       " 'hopes': 42,\n",
       " 'will': 43,\n",
       " 'take': 44,\n",
       " 'lessons': 45,\n",
       " 'learned': 46,\n",
       " 'immersion': 47,\n",
       " 'with': 48,\n",
       " 'where': 49,\n",
       " 'do': 50,\n",
       " 'go': 51,\n",
       " 'from': 52,\n",
       " 'there': 53}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f1c26c",
   "metadata": {},
   "source": [
    "### Generating text using NGrams "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "23a32356",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e4428de1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Time</th>\n",
       "      <th>Tweet_Text</th>\n",
       "      <th>Type</th>\n",
       "      <th>Media_Type</th>\n",
       "      <th>Hashtags</th>\n",
       "      <th>Tweet_Id</th>\n",
       "      <th>Tweet_Url</th>\n",
       "      <th>twt_favourites_IS_THIS_LIKE_QUESTION_MARK</th>\n",
       "      <th>Retweets</th>\n",
       "      <th>Unnamed: 10</th>\n",
       "      <th>Unnamed: 11</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16-11-11</td>\n",
       "      <td>15:26:37</td>\n",
       "      <td>Today we express our deepest gratitude to all ...</td>\n",
       "      <td>text</td>\n",
       "      <td>photo</td>\n",
       "      <td>ThankAVet</td>\n",
       "      <td>7.970000e+17</td>\n",
       "      <td>https://twitter.com/realDonaldTrump/status/797...</td>\n",
       "      <td>127213</td>\n",
       "      <td>41112</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16-11-11</td>\n",
       "      <td>13:33:35</td>\n",
       "      <td>Busy day planned in New York. Will soon be mak...</td>\n",
       "      <td>text</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.970000e+17</td>\n",
       "      <td>https://twitter.com/realDonaldTrump/status/797...</td>\n",
       "      <td>141527</td>\n",
       "      <td>28654</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16-11-11</td>\n",
       "      <td>11:14:20</td>\n",
       "      <td>Love the fact that the small groups of protest...</td>\n",
       "      <td>text</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.970000e+17</td>\n",
       "      <td>https://twitter.com/realDonaldTrump/status/797...</td>\n",
       "      <td>183729</td>\n",
       "      <td>50039</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16-11-11</td>\n",
       "      <td>2:19:44</td>\n",
       "      <td>Just had a very open and successful presidenti...</td>\n",
       "      <td>text</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.970000e+17</td>\n",
       "      <td>https://twitter.com/realDonaldTrump/status/796...</td>\n",
       "      <td>214001</td>\n",
       "      <td>67010</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16-11-11</td>\n",
       "      <td>2:10:46</td>\n",
       "      <td>A fantastic day in D.C. Met with President Oba...</td>\n",
       "      <td>text</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.970000e+17</td>\n",
       "      <td>https://twitter.com/realDonaldTrump/status/796...</td>\n",
       "      <td>178499</td>\n",
       "      <td>36688</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Date      Time                                         Tweet_Text  \\\n",
       "0  16-11-11  15:26:37  Today we express our deepest gratitude to all ...   \n",
       "1  16-11-11  13:33:35  Busy day planned in New York. Will soon be mak...   \n",
       "2  16-11-11  11:14:20  Love the fact that the small groups of protest...   \n",
       "3  16-11-11   2:19:44  Just had a very open and successful presidenti...   \n",
       "4  16-11-11   2:10:46  A fantastic day in D.C. Met with President Oba...   \n",
       "\n",
       "   Type Media_Type   Hashtags      Tweet_Id  \\\n",
       "0  text      photo  ThankAVet  7.970000e+17   \n",
       "1  text        NaN        NaN  7.970000e+17   \n",
       "2  text        NaN        NaN  7.970000e+17   \n",
       "3  text        NaN        NaN  7.970000e+17   \n",
       "4  text        NaN        NaN  7.970000e+17   \n",
       "\n",
       "                                           Tweet_Url  \\\n",
       "0  https://twitter.com/realDonaldTrump/status/797...   \n",
       "1  https://twitter.com/realDonaldTrump/status/797...   \n",
       "2  https://twitter.com/realDonaldTrump/status/797...   \n",
       "3  https://twitter.com/realDonaldTrump/status/796...   \n",
       "4  https://twitter.com/realDonaldTrump/status/796...   \n",
       "\n",
       "   twt_favourites_IS_THIS_LIKE_QUESTION_MARK  Retweets  Unnamed: 10  \\\n",
       "0                                     127213     41112          NaN   \n",
       "1                                     141527     28654          NaN   \n",
       "2                                     183729     50039          NaN   \n",
       "3                                     214001     67010          NaN   \n",
       "4                                     178499     36688          NaN   \n",
       "\n",
       "   Unnamed: 11  \n",
       "0          NaN  \n",
       "1          NaN  \n",
       "2          NaN  \n",
       "3          NaN  \n",
       "4          NaN  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"Donald-Tweets!.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d824874b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize, sent_tokenize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7056b739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word tokenization\n",
    "\n",
    "trump_corpus = list(df['Tweet_Text'].apply(word_tokenize))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa21a561",
   "metadata": {},
   "source": [
    "#### Padding ensures that each symbol of the actual string occurs at all positions of the ngram. \n",
    " Eg; For a given word 'TEXT',\n",
    " \n",
    " bi-grams _T, TE, EX, XT, T_\n",
    " \n",
    " tri-grams _TE, TEX, EXT, XT_, T__\n",
    " \n",
    " quad-grams _TEX, TEXT, EXT_, XT__, T___\n",
    " \n",
    "#### And then, to create the vocabulary we need to pad our sentences (just like for counting ngrams) and then combine the sentences into one flat stream of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ebe18783",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.lm.preprocessing import padded_everygram_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9910ab62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the tokenized text for 3-grams language modelling\n",
    "\n",
    "n = 3\n",
    "train_data, padded_sents = padded_everygram_pipeline(n, trump_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "214657f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.lm import MLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "50d5ff83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the model using Maximum Likelihood\n",
    "\n",
    "trump_model = MLE(n) # Lets train a 3-grams model, previously we set n=3\n",
    "trump_model.fit(train_data, padded_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "966d6c3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', '<s>', 'We', 'are', 'going', 'to', 'MAKE', 'AMERICA', 'GREAT', 'AGAIN', '!', '#', 'MakeAmericaGreatAgain', 'https', ':', '//t.co/DrVa65X9rI', '</s>', '</s>', '</s>', '</s>']\n"
     ]
    }
   ],
   "source": [
    "print(trump_model.generate(20, random_seed=7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "29c8d6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize.treebank import TreebankWordDetokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "16f58f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dekonize to represent it as a normal sentence\n",
    "\n",
    "detokenize = TreebankWordDetokenizer().detokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "21cd3869",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function to generate sentence\n",
    "\n",
    "def generate_sent(model, num_words, random_seed=42):\n",
    "    content = []\n",
    "    for token in model.generate(num_words, random_seed=random_seed): #num_words: Max words to generate\n",
    "        if token == '<s>':\n",
    "            continue\n",
    "        if token == '</s>':\n",
    "            break\n",
    "        content.append(token)\n",
    "    return detokenize(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "06174e32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'via my Facebook page in St. Joseph, Michigan . Streaming live - join us today because of my constant'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_sent(trump_model, num_words=20, random_seed=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5cfacb82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "will MAKE AMERICA GREAT AGAIN\n"
     ]
    }
   ],
   "source": [
    "print(generate_sent(trump_model, num_words=5, random_seed=52))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
